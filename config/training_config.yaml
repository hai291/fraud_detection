training:

  learning_rate: 5e-4         
  batch_size: null               
  num_epochs: 200              
  warmup_steps: 0             
  weight_decay: 1e-4          

  
  gradient_accumulation_steps: 1
  max_grad_norm: null            

  scheduler_type: null          


  optimizer: "adam"          
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1e-8


  logging_steps: 20             
  eval_steps: 20                 


  early_stopping_threshold: 0.001
  early_stopping_patience: 10





